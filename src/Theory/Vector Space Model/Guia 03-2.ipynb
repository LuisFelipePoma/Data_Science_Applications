{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BxZ17_8d5sz"
   },
   "source": [
    "# 04 - Preprocesamiento de textos (Normalización)\n",
    "\n",
    "* Antes de procesar los texto con cualquier algoritmo de aprendizaje automático (supervisado o no supervisado) es necesario realizar un preporcesamiento con el objetivo de limpiar, normalizar y estructurar el texto.\n",
    "\n",
    "\n",
    "* Para ello se propone el siguiente framework:\n",
    "\n",
    "\n",
    "* Los pasos propuestos en este framework pueden abordarse en el orden que se quiera e incluso alguno de estas etapas no sería necesario realizarse en función de como tengamos los textos.\n",
    "\n",
    "\n",
    "* Definamos a continuación lo que hay que realizar en cada uno de estos pasos:\n",
    "\n",
    "\n",
    "1.- ***Eliminación de ruido***: \n",
    "\n",
    "   * Este paso tiene como objetivo eliminar todos aquellos símbolos o caracteres que no aportan nada en el significado de las frases (ojo no confundir con las stop-words), como por ejemplo etiquetas HTML (para el caso del scraping), parseos de XML, JSON, etc.\n",
    "    \n",
    "2.- ***Tokenización***: \n",
    "   * Este paso tiene como objetivo dividir las cadenas de texto del documento en piezas más pequeñas o tokens.\n",
    "   * Aunque la tokenización es el proceso de dividir grandes cadenas de texto en cadenas más pequeñas, se suele diferenciar la:\n",
    "       * ***Segmentation***: Tarea de dividir grandes cadenas de texto en piezas más pequeñas como oraciones o párrafos.\n",
    "       * ***Tokenization***: Tarea de dividir grandes cadenas de texto solo y exclusivamente en palabras.\n",
    "    \n",
    "3.- ***Normalización***:\n",
    "\n",
    "   * La normalización es una tarea que tiene como objetivo poner todo el texto en igualdad de condiciones:\n",
    "        * Convertir todo el texto en mayúscula o minúsculas\n",
    "        * Eliminar, puntos, comas, comillas, etc.\n",
    "        * Convertir los números a su equivalente a palabras\n",
    "        * Quitar las Stop-words\n",
    "        * etc.\n",
    "        \n",
    "<hr>\n",
    "\n",
    "## Ejemplo de Preprocesamiento de Texto.\n",
    "\n",
    "\n",
    "* Aunque no hay una norma o guía de como realizar una normalización de texto ya que esta depende del problema a resolver y de la naturaleza del texto, vamos a mostrar a continuación algunas operaciones más o menos comúnes para la tokenización y normalización de los textos.\n",
    "\n",
    "\n",
    "* Si bien este ejemplo esta hecho utilizando la librería de ***spaCy*** (ya que lo vamos a aplicar sobre un texto en Español) puede realizarse tambien con la librería de ***NLTK*** e incluso determinadas funcionalidades de tratamiento de strings lo podemos hacer con otras librerías.\n",
    "\n",
    "\n",
    "* En el siguiente ejemplo vamos a tokenizar y normalizar un texto:\n",
    "    1. Transformar un texto en tokens\n",
    "    2. Eliminar los tokens que son signos (puntuación, exclamación, etc.)\n",
    "    3. Eliminar las palabras que tienen menos de 'N' caracteres\n",
    "    4. Eliminar las palabras que son Stop Words\n",
    "    5. Pasar el texto a minúsculas\n",
    "    6. Lematización\n",
    "    \n",
    "    \n",
    "* **Nota**: *la normalización de texto que se va a codificar a continuación puede codificarse de forma más optimizada sin la necesidad de recorrer tantas veces la lista de tokens. Ya que este es un ejemplo con fines didácticos, este se centra en los conceptos y no en la optimización*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rQfLhrYukxiq"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "azpg_ggxd5s7"
   },
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    \"\"\"\n",
    "    Función que dado un texto devuelve una lista con las palabras del texto no vacias\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [word.text.strip() for word in doc if len(word.text.strip()) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MCDWVZwfd5s8"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    \"\"\"\n",
    "    Función que dada una lista de palabras, elimina los signos de puntuación\n",
    "    \"\"\"\n",
    "    doc = spacy.tokens.doc.Doc(nlp.vocab, words=words)\n",
    "    return [word.text for word in doc if not word.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cJMWK3lSd5s8"
   },
   "outputs": [],
   "source": [
    "def remove_short_words(words, num_chars):\n",
    "    \"\"\"\n",
    "    Función que dada una lista de palabras y un número mínimo de caracteres que tienen que tener\n",
    "    las palabras, elimina todas las palabras que tengan menos caracteres que los indicados\n",
    "    \"\"\"\n",
    "    return [word for word in words if len(word) > num_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ik34eTGTd5s9"
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    \"\"\"\n",
    "    Función que dada una lista de palabras, elimina las Stop Words\n",
    "    \"\"\"\n",
    "    doc = nlp(\" \".join(words))\n",
    "    return [word.text for word in doc if not word.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nZhunQTTd5s9"
   },
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"\n",
    "    Función que dada una lista de palabras, las transforma a minúsculas\n",
    "    \"\"\"\n",
    "    return [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "doVHk8wGd5s9"
   },
   "outputs": [],
   "source": [
    "def lemmatizer(words):\n",
    "    \"\"\"\n",
    "    Función que dada una lista de palabras, devuelve esa lista con el lema de cada una de esas palabras\n",
    "    \"\"\"\n",
    "    doc = nlp(\" \".join(words))\n",
    "    return [word.lemma_ for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ffr33YE5d5s-"
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    Dado un texto, devuelve el texto tokenizado y normalizado\n",
    "    \"\"\"\n",
    "    words = get_tokens(text=text)\n",
    "    words = remove_punctuation(words=words)\n",
    "    words = remove_short_words(words=words, num_chars=3)\n",
    "    words = remove_stop_words(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = lemmatizer(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr83_Easd5s-"
   },
   "source": [
    "#### Pasamos a tokenizar y normalizar el siguiente texto usando la función de normalización realizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tm8GXIXtd5s_",
    "outputId": "2770a7e9-ff9c-4f9d-99ba-c56a91298115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sector', 'cultural', 'golpeado', 'pandemiar', 'cantidad', 'espectáculo', 'cancelado', 'gracias', 'virtualidad', 'seguir', 'promover', 'iniciativa', 'cultural', 'valer él', 'posibilidad', 'nacer', 'cultural', 'centro', 'cultural', 'surgir', 'propósito', 'democratizar', 'cultura', 'arte', 'enero', '2022', 'cumplir', 'motivo', 'aniversario', 'conversatorio', 'gestión', 'cultural', 'pandemio', 'evento', 'ponentes', 'sergio', 'llusera', 'director', 'centro', 'cultural', 'universidad', 'pacífico', 'maría', 'eugenia', 'yllia', 'magíster', 'museología', 'universidad', 'ricardo', 'palma', 'contar', 'participación', 'carín', 'moreno', 'docente', 'carrera', 'art', 'escénico']\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"El sector cultural fue uno de los más golpeados en esta pandemia, debido a la gran cantidad de espectáculos que fueron cancelados.\n",
    "Sin embargo, gracias a la virtualidad fue posible seguir promoviendo iniciativas culturales.\n",
    "Valiéndose de esta posibilidad, nació el año pasado UPC Cultural, el centro cultural que surgió con el claro propósito de democratizar la cultura y el arte.\n",
    "Y en enero de este 2022 cumplió su primer año.\n",
    "Con motivo de su aniversario, se realizó el conversatorio ‘La gestión cultural antes, durante y después de la pandemia’,\n",
    "evento que tuvo como ponentes a Sergio Llusera, director del Centro Cultural de la Universidad del Pacífico, y María Eugenia Yllia, magíster en Museología\n",
    "por la Universidad Ricardo Palma. Además, contó con la participación de Carina Moreno, docente de la carrera de Artes Escénicas de la UPC.\"\"\"\n",
    "print(normalize(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4sG3H02d5tA"
   },
   "source": [
    "#### En este ejemplo podemos ver como reducimos las palabras (tokens) del texto original, quedandonos con lo importante y normalizado\n",
    "#### Pasamos de 156 tokens del texto original a 58 tokens tras la normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ZvY4wk0d5tA",
    "outputId": "d1ef9b6d-24d8-4f71-9178-2e047cead23e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tokens del texto original: 156\n",
      "Número de tokens distintos del texto original: 97\n",
      "Número de tokens tras la normalización: 58\n",
      "Número de tokens distintos tras la normalización: 51\n"
     ]
    }
   ],
   "source": [
    "print('Número de tokens del texto original: ' + str(len(get_tokens(raw))))\n",
    "print('Número de tokens distintos del texto original: ' + str(len(set(get_tokens(raw)))))\n",
    "print('Número de tokens tras la normalización: ' + str(len(normalize(raw))))\n",
    "print('Número de tokens distintos tras la normalización: ' + str(len(set(normalize(raw)))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "04_Preprocesamiento_de_textos_Normalizacion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
