{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXjOv0sB_N8b"
      },
      "source": [
        "### Uso de NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SACapOdX9sQ8",
        "outputId": "e895f38a-5a55-4ae0-b1ca-2a8aa64dd3d9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "#descargar el corpus en español\n",
        "nltk.download(\"cess_esp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTIBg5zG_UeZ"
      },
      "source": [
        "### Expresiones regulares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85rav7DS9-z9",
        "outputId": "6cfd2846-2e8e-42e6-8bc8-12a0f0fc924c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]\n",
            "6030\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "corpus = nltk.corpus.cess_esp.sents()\n",
        "\n",
        "print(corpus)\n",
        "print(len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O0bP1_OCAs6x"
      },
      "outputs": [],
      "source": [
        "# Aplanar una lista compuesta de sublistas\n",
        "flatten = [w for l in corpus for w in l]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_vp-Wq7BhVq",
        "outputId": "e2e1f227-4f46-410b-b403-679802d85468"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "192686"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(flatten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98m5FjA-Bq0d",
        "outputId": "dd5b75bb-8916-4d3c-e1b4-4c305614f79c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana']\n"
          ]
        }
      ],
      "source": [
        "print(flatten[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2rghBc8CFNK"
      },
      "source": [
        "Aplicacion de expresiones regulares sobre la variable **flatten**:\n",
        "\n",
        "re.search(p,s)\n",
        "\n",
        "p = patron de busqueda\n",
        "s = cadena donde buscar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIjtvgOFCMX2",
        "outputId": "ed8f9abc-9686-44b4-b883-b2cc4d1038f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['estatal', 'jueves', 'empresa', 'centrales', 'francesa']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Meta-caracteres basicos\n",
        "\n",
        "arr = [w for w in flatten if re.search(\"es\", w)]\n",
        "\n",
        "arr[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rguHBFA1DcnO",
        "outputId": "ef4251e7-aa7b-4842-f0c0-10d9e6cf361b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['jueves', 'centrales', 'millones', 'millones', 'dólares']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arr = [w for w in flatten if re.search(\"es$\", w)]\n",
        "\n",
        "arr[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf0timi3DqWt",
        "outputId": "357dd587-e314-4990-e056-8b7c279b4081"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tajantes']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arr = [w for w in flatten if re.search(\"^..j..t..$\", w)]\n",
        "\n",
        "arr[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEV2X6vLEUxR",
        "outputId": "dea4202b-4f18-4e1e-b02c-97a3326e7ad7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['golf', 'golf']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Aplicacion de rangos\n",
        "# [a-z], [A-Z], [0-9]\n",
        "arr = [w for w in flatten if re.search(\"^[ghi][mno][jlk][def]$\", w)]\n",
        "\n",
        "arr[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSAxM7TN88XV"
      },
      "source": [
        "## Normalizacion del Texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiR8tDZ-92Nf"
      },
      "source": [
        "(1) Tokenizacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7zDPPIS-VCe",
        "outputId": "886762e5-109c-40b7-c274-7cf6b2412712"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuando sea presidente del mundo (imaginaba en mi cabeza) no tendre que preocuparme por tantas tonteras.\n",
            "Era solo un niño de 10 años, pero pensaba que podria ser cualquier cosa en su imaginacion...\n"
          ]
        }
      ],
      "source": [
        "texto = \"\"\"Cuando sea presidente del mundo (imaginaba en mi cabeza) no tendre que preocuparme por tantas tonteras.\n",
        "Era solo un niño de 10 años, pero pensaba que podria ser cualquier cosa en su imaginacion...\"\"\"\n",
        "\n",
        "print(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42pz-deq_RSd",
        "outputId": "b2a24cb0-9280-4c10-98e6-f891c67bc76d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', '(imaginaba', 'en', 'mi', 'cabeza)', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras.\\nEra', 'solo', 'un', 'niño', 'de', '10', 'años,', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion...']\n"
          ]
        }
      ],
      "source": [
        "#Caso 1\n",
        "print(re.split(r' ', texto))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXCUO-_J_8Fo",
        "outputId": "f4e9cfe9-2a49-4b44-d201-2871862f1f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', '(imaginaba', 'en', 'mi', 'cabeza)', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras.', 'Era', 'solo', 'un', 'niño', 'de', '10', 'años,', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion...']\n"
          ]
        }
      ],
      "source": [
        "#Caso 2:\n",
        "print(re.split(r'[ \\t\\n]+', texto))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rd1GqRxAaVp",
        "outputId": "a51f8b08-b123-4b23-a3bd-9bced5071f6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', 'imaginaba', 'en', 'mi', 'cabeza', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras', 'Era', 'solo', 'un', 'niño', 'de', '10', 'años', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion', '']\n"
          ]
        }
      ],
      "source": [
        "#Caso 3:\n",
        "# \\W omite todos los caracteres que nos sean letras, digitos o guiones\n",
        "print(re.split(r'[ \\W\\t\\n]+', texto))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JRxhZQ_9ARl",
        "outputId": "2f07f566-6f35-4428-8bf8-19ab01f5216b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['En', 'los', 'E', 'U', 'esa', 'hamburguesa', 'cuesta', '15', '50']\n"
          ]
        }
      ],
      "source": [
        "texto = \"En los E.U. esa hamburguesa cuesta $15.50\"\n",
        "print(re.split(r'[ \\W\\t\\n]+',texto))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JMFBeGqhBi5f"
      },
      "outputs": [],
      "source": [
        "pattern = r'''(?x)                 # set flag to allow verbose regexps\n",
        "              (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n",
        "              | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
        "              | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
        "              | \\.\\.\\.             # ellipsis\n",
        "              | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLYphGDNDCdS",
        "outputId": "a19c363e-8e6a-454a-94f9-c06384f7a03e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['En', 'los', 'E.U.', 'esa', 'hamburguesa', 'cuesta', '$15.50']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.regexp_tokenize(texto, pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqZ7vvrAEIAk"
      },
      "source": [
        "(2) Lematizacion (encontrar la raiz linguistica de una palabra)\n",
        "\n",
        "Stemming = lematizacion simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-WJG9u5EPlV",
        "outputId": "205d0d90-b66a-4dfa-f4e3-9603ae9f4323"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "SnowballStemmer.languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8NBaDQrFFHrp"
      },
      "outputs": [],
      "source": [
        "stem = SnowballStemmer('spanish')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZYBKglCEFQC3",
        "outputId": "8b59b6f3-5736-47c4-f222-717c86c6e9ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'trabaj'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stem.stem(\"trabajaremos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oFLQxiP1Fqmf"
      },
      "outputs": [],
      "source": [
        "#Lematizacion\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemm= WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0PQVEVvF8Cl",
        "outputId": "268a5059-130b-479b-dd56-90c629ccaf3c"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7jz9kHSgGK12",
        "outputId": "8d2c1c13-30f5-4e48-d720-ea1f6cb20972"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'trabajó'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lemm.lemmatize('trabajó')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjoF-eNiGn1d"
      },
      "source": [
        "## Uso de Spacy en NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrU39xqKTR-R"
      },
      "source": [
        "**spacy** es una librería de procesamiento del lenguaje natural, robusta, rápida, fácil de instalar y utilizar e integrable con otras librerías de NLP y de deep learning.\n",
        "\n",
        "Tiene modelos entrenados en varios idiomas y permite realizar las típicas tareas de segmentación por oraciones, tokenizanción, análisis morfológico, extracción de entidades y análisis de opinión.\n",
        "\n",
        "Una vez instalados los modelos, podemos importarlos fácilmente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKYCTJ_8GyIs",
        "outputId": "44b011a8-3178-4011-c7be-b5f2fd95b921"
      },
      "outputs": [],
      "source": [
        "%pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVIJYU9eG7s0",
        "outputId": "2cd03755-bd48-4d15-a638-d66b260cf00e"
      },
      "outputs": [],
      "source": [
        "#Descargamos el modelo de Spacy en español\n",
        "!python -m spacy download es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OFe2jbIeHSos"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gaCumO75HZYz"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('es_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YPUUszhzIneC"
      },
      "outputs": [],
      "source": [
        "text = \"Soy un texto.  Normalmente soy más largo y más grande.  Que no te engañe mi tamaño mayor a 10 palabras\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1nvb-jGgI7BM"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-UerA9zJA-s",
        "outputId": "500c56de-e31c-4a8f-d90d-90cde4ce60f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Soy',\n",
              " 'un',\n",
              " 'texto',\n",
              " '.',\n",
              " ' ',\n",
              " 'Normalmente',\n",
              " 'soy',\n",
              " 'más',\n",
              " 'largo',\n",
              " 'y',\n",
              " 'más',\n",
              " 'grande',\n",
              " '.',\n",
              " ' ',\n",
              " 'Que',\n",
              " 'no',\n",
              " 'te',\n",
              " 'engañe',\n",
              " 'mi',\n",
              " 'tamaño',\n",
              " 'mayor',\n",
              " 'a',\n",
              " '10',\n",
              " 'palabras']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Creamos una lista con los tokens del texto\n",
        "tokens = [t.orth_ for t in doc]\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZPwEl6iJyar"
      },
      "source": [
        "**Limpiando el texto en Spacy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdPFIOXKgrN"
      },
      "source": [
        "Omitir:\n",
        "\n",
        "- Palabras comunes (y, o, ni, que)\n",
        "- Preposiciones (a, en, para, por, entre, otras)\n",
        "- Verbos (ser)\n",
        "- Las puntuaciones\n",
        "\n",
        "Todas aquellas palabras que no aportan un significado importante en el texto las denominaremos stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmr2Sd5sKutb",
        "outputId": "be95d3f8-d992-4bc7-8715-bb2b9274f374"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['texto', 'Normalmente', 'engañe', 'tamaño', '10', 'palabras']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_validos = [t.orth_ for t in doc if (not t.is_punct | t.is_stop) and t.orth_ != ' ']\n",
        "tokens_validos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXidc8udKmrk"
      },
      "source": [
        "**Normalizar texto en Spacy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLra4LqmJ2pr",
        "outputId": "40070884-ed92-47bf-b90d-3e8b6a74d68a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['texto', 'normalmente', 'engañe', 'tamaño', 'palabras']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = [t.lower() for t in tokens_validos if len(t)>3 and t.isalpha()]\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr7CNuUcXlm-"
      },
      "source": [
        "## **Similitud semántica entre palabras, frases y documentos**\n",
        "\n",
        "spaCy permite calcular la similitud semántica entre cualquier par de objetos de tipo Doc, Span o Token.\n",
        "\n",
        "Ojo, La similitud semántica es un concepto algo subjetivo, pero en este caso se puede entender como la probabilidad de que dos palabras aparezcan en los mismos contextos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "a90n_8y3W_bO"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fCwXNGTnXAzP"
      },
      "outputs": [],
      "source": [
        "nlp_en = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUjA16sFYPWk",
        "outputId": "049669ca-4dc9-4548-c9a4-961cba66632c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cats vs dogs 0.34804314374923706\n",
            "research vs development 0.1919044852256775\n",
            "cats vs development 0.14609646797180176\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\pms_l\\AppData\\Local\\Temp\\ipykernel_32348\\1568170380.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token2, token1.similarity(token2))\n",
            "C:\\Users\\pms_l\\AppData\\Local\\Temp\\ipykernel_32348\\1568170380.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token3, \"vs\", token4, token3.similarity(token4))\n",
            "C:\\Users\\pms_l\\AppData\\Local\\Temp\\ipykernel_32348\\1568170380.py:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token4, token1.similarity(token4))\n"
          ]
        }
      ],
      "source": [
        "# analizamos algunas colocaciones en inglés\n",
        "token1, _, token2 = nlp_en(\"cats and dogs\")\n",
        "token3, _, token4 = nlp_en(\"research and development\")\n",
        "\n",
        "# medimos la similitud semántica entre algunos pares\n",
        "print(token1, \"vs\", token2, token1.similarity(token2))\n",
        "print(token3, \"vs\", token4, token3.similarity(token4))\n",
        "print(token1, \"vs\", token4, token1.similarity(token4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yac1-xYYPkT",
        "outputId": "eedb41ab-8a65-41a8-bd71-ba02f16147cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "perros vs gatos 0.6648734211921692\n",
            "investigación vs desarrollo 0.182033509016037\n",
            "perros vs desarrollo -0.03289707377552986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\pms_l\\AppData\\Local\\Temp\\ipykernel_32348\\314909809.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token2, token1.similarity(token2))\n",
            "C:\\Users\\pms_l\\AppData\\Local\\Temp\\ipykernel_32348\\314909809.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token3, \"vs\", token4, token3.similarity(token4))\n",
            "C:\\Users\\pms_l\\AppData\\Local\\Temp\\ipykernel_32348\\314909809.py:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token4, token1.similarity(token4))\n"
          ]
        }
      ],
      "source": [
        "# ¿qué tal funciona en español?\n",
        "token1, _, token2 = nlp(\"perros y gatos\")\n",
        "token3, _, token4 = nlp(\"investigación y desarrollo\")\n",
        "\n",
        "# medimos la similitud semántica entre algunos pares\n",
        "print(token1, \"vs\", token2, token1.similarity(token2))\n",
        "print(token3, \"vs\", token4, token3.similarity(token4))\n",
        "print(token1, \"vs\", token4, token1.similarity(token4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
